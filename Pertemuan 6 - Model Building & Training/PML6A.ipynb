{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9a7696",
   "metadata": {},
   "source": [
    "<b>Pertemuan 6 Lab PML // SAMPLE</b> <br>\n",
    "&#169; Gary Alvaro\n",
    "\n",
    "<b style=\"color: red\">Pastikan teman-teman sudah coba menjalankan file ini sebelum praktikum dimulai, dan pastikan semua libraries sudah terinstall. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4516c",
   "metadata": {},
   "source": [
    "Silahkan download file di bawah dan letakkan di folder/direktori yang sama dengan file notebook ini.\n",
    "\n",
    "<a href=\"https://drive.google.com/file/d/126WDCZMZNujwRaxAs-Y9AvvoSUDfcIX3/view?usp=share_link\" target=\"_blank\">https://drive.google.com/file/d/126WDCZMZNujwRaxAs-Y9AvvoSUDfcIX3/view?usp=share_link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a9770",
   "metadata": {},
   "source": [
    "## Import Libraries yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52602e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\GARY\n",
      "[nltk_data]     ALVARO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Loading Bar TQDM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Stopword Removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Stemming (Sastrawi)\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pickle FastText\n",
    "import pickle\n",
    "\n",
    "# Split Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Building\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from keras.models import load_model\n",
    "\n",
    "# Callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Grafik\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b17d3",
   "metadata": {},
   "source": [
    "## Mendefinisikan Variabel Penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e59c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSize = 0.15 #Pembagian ukuran datatesing\n",
    "\n",
    "MAX_NB_WORDS = 100000 #Maximum jumlah kata pada vocabulary yang akan dibuat\n",
    "max_seq_len = 46 #Panjang kalimat maximum\n",
    "\n",
    "num_epochs = 40 #Jumlah perulangan / epoch saat proses training\n",
    "\n",
    "aspek_kategori = ['akurasi', 'kualitas', 'pelayanan', 'pengemasan', 'harga', 'pengiriman']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a1ca6",
   "metadata": {},
   "source": [
    "## Read & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05e8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    # Membaca file CSV ke dalam Dataframe\n",
    "    df = pd.read_csv(filename) \n",
    "    \n",
    "    # Encoding Categorical Data (Mengubah data kategorikal menjadi angka)\n",
    "    label_dict = {'negatif':0, 'positif':1, '-':99}\n",
    "    df['s_akurasi'] = df['s_akurasi'].replace(label_dict)\n",
    "    df['s_kualitas'] = df['s_kualitas'].replace(label_dict)\n",
    "    df['s_pelayanan'] = df['s_pelayanan'].replace(label_dict)\n",
    "    df['s_pengemasan'] = df['s_pengemasan'].replace(label_dict)\n",
    "    df['s_harga'] = df['s_harga'].replace(label_dict)\n",
    "    df['s_pengiriman'] = df['s_pengiriman'].replace(label_dict)    \n",
    "    \n",
    "    # Membagi dataframe menjadi data training & testing\n",
    "    df_training, df_testing = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # Reset Index\n",
    "    df_training = df_training.reset_index()\n",
    "    df_testing = df_testing.reset_index()\n",
    "\n",
    "    return df_training, df_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45a694",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Kali ini, preprocessing akan dibuat dalam bentuk singkat menggunakan fungsi.\n",
    "\n",
    "Lakukan pendefenisian fungsi dan pemanggilan fungsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26b79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    # Case Folding\n",
    "    data['lower'] = data['teks'].str.lower()\n",
    "    \n",
    "    # Punctual Removal\n",
    "    data['punctual'] = data['lower'].str.replace('[^a-zA-Z]+',' ', regex=True)\n",
    "    \n",
    "    # Normalization\n",
    "    kamus_baku = pd.read_csv('../Pertemuan 4 - Preprocessing NLP/kata_baku.csv', sep=\";\")\n",
    "    dict_kamus_baku = kamus_baku[['slang','baku']].to_dict('list')\n",
    "    dict_kamus_baku = dict(zip(dict_kamus_baku['slang'], dict_kamus_baku['baku']))\n",
    "    norm = []\n",
    "    for i in data['punctual']:\n",
    "        res = \" \".join(dict_kamus_baku.get(x, x) for x in str(i).split())\n",
    "        norm.append(str(res))\n",
    "    data['normalize'] = norm\n",
    "    \n",
    "    # Stopword Removal\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    swr = []\n",
    "    for i in tqdm(data['normalize']):\n",
    "        tokens = word_tokenize(i)\n",
    "        filtered = [word for word in tokens if word not in stop_words]\n",
    "        swr.append(\" \".join(filtered))\n",
    "    data['stopwords'] = swr\n",
    "    \n",
    "    # Stemming\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    stem = []\n",
    "    for i in tqdm(data['stopwords']):\n",
    "        stem.append(stemmer.stem(str(i)))\n",
    "    data['stemmed'] = stem\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cd6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pemanggilan fungsi read_data\n",
    "\n",
    "filename = \"../Pertemuan 4 - Preprocessing NLP/DatasetReviewProduk-LabPML.csv\"\n",
    "df_training, df_testing = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775de73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1200/1200 [00:00<00:00, 12699.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1200/1200 [01:21<00:00, 14.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 19217.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 300/300 [00:24<00:00, 12.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pemanggilan fungsi preprocessing\n",
    "\n",
    "df_training = preprocessing(df_training)\n",
    "df_testing = preprocessing(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95fcae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "kali komplain kali kasih barang rusak driver grab salah kirim barang nunggu kirim komplain suruh grab kembali barang salah salah lapak diskusi komplain server error tramsaksi bl selesai dana terus lapak transaksi masalah komplain chat pelapam kirim ganti barang tes rusak komplain kali rusak terima kecewa\n"
     ]
    }
   ],
   "source": [
    "# Mengecek panjang kalimat maksimum\n",
    "\n",
    "longest_string = max(df_training['stemmed'].values.tolist(), key=len)\n",
    "print(len(longest_string.split()))\n",
    "\n",
    "print(longest_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca03aeb",
   "metadata": {},
   "source": [
    "## Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52dd2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi Training\n",
    "def tokenize_training(data_training):\n",
    "    global tokenizer #Menggunakan variabel global agar 'tokenizer' bisa dipake di luar fungsi ini\n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS, char_level=False)\n",
    "    tokenizer.fit_on_texts(data_training['stemmed'])\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    train_sequences = tokenizer.texts_to_sequences(data_training['stemmed'])\n",
    "    word_seq_train = pad_sequences(train_sequences, maxlen = max_seq_len)\n",
    "\n",
    "    return word_index, word_seq_train\n",
    "\n",
    "\n",
    "# Tokenisasi Testing\n",
    "def tokenize_testing(data_testing):\n",
    "    test_sequences = tokenizer.texts_to_sequences(data_testing['stemmed'])\n",
    "    word_seq_test = pad_sequences(test_sequences, maxlen = max_seq_len)\n",
    "\n",
    "    return word_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa571855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi secara umum\n",
    "def tokenize(data):\n",
    "    sequences = tokenizer.texts_to_sequences(data['stemmed'])\n",
    "    word_seq = pad_sequences(sequences, maxlen = max_seq_len)\n",
    "\n",
    "    return word_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9644fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pemanggilan Fungsi Tokenisasi\n",
    "\n",
    "word_index, word_seq_train = tokenize_training(df_training)\n",
    "word_seq_test = tokenize_testing(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d736541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
